{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import itk\n",
    "import tifffile\n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import colorcet as cc\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "\n",
    "### Set the colormap\n",
    "obj = 'cet_diverging_bwr_20_95_c54'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Paths to Calibration Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to update these manually as more calibration experiments are conducted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You should change this path to reflect your owk working directory\n",
    "ddir = f'/path/to/Boaz/I2/20240930_iDISCO_round2/'\n",
    "calims_552 = ['552101','552102','550750']\n",
    "calims_552 = [os.path.join(ddir,'ANM'+st+'_JF552') for st in calims_552]\n",
    "calims_673 = ['549895','552100','555643']\n",
    "calims_673 = [os.path.join(ddir,'ANM'+st+'_JFX673') for st in calims_673]\n",
    "\n",
    "### You should change this path to reflect your owk working directory\n",
    "ddir_r1 = f'/path/to/Boaz/I2/2024-09-19_iDISCO_CalibrationBrains/'\n",
    "calims_552_r1 = ['549057_left','550749_left']\n",
    "calims_552_r1 = [os.path.join(ddir_r1,'ANM'+st+'_JF552') for st in calims_552_r1]\n",
    "calims_673_r1 = ['550751_left','551089_left']\n",
    "calims_673_r1 = [os.path.join(ddir_r1,'ANM'+st+'_JF673') for st in calims_673_r1]\n",
    "\n",
    "### Additional directories and animal monikers can be added below, following the template above\n",
    "\n",
    "###\n",
    "\n",
    "### You should add the directories, path lists, and animal lsits to the following, as necessary\n",
    "dirs = [ddir,ddir_r1]\n",
    "calims_552_tot = calims_552 + calims_552_r1\n",
    "calims_673_tot = calims_673 + calims_673_r1\n",
    "\n",
    "ANMS_552 = [anm.split('/')[-1] for anm in calims_552_tot]\n",
    "ANMS_673 = [anm.split('/')[-1] for anm in calims_673_tot]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detemrine number of regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking the size of the region_csv to initialize the array\n",
    "fname = os.path.join(calims_552_tot[0],'itk/region_stats.csv')\n",
    "data = np.loadtxt(fname,delimiter=',',skiprows=1)\n",
    "regions = data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the region stat arrays for both channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_stats_552 = np.zeros((len(calims_552_tot),regions.shape[0],5))\n",
    "for j, path in zip(range(len(calims_552_tot)),calims_552_tot):\n",
    "    fname = os.path.join(path,'itk/region_stats.csv')\n",
    "    data = np.loadtxt(fname,delimiter=',',skiprows=1)\n",
    "    regions = data[:,0]\n",
    "    area = data[:,1]\n",
    "    chAF = data[:,2]\n",
    "    ch552 = data[:,5]\n",
    "    ch673 = data[:,8]\n",
    "    region_stats_552[j,:,0] = regions\n",
    "    region_stats_552[j,:,1] = chAF\n",
    "    region_stats_552[j,:,2] = ch552\n",
    "    region_stats_552[j,:,3] = ch673\n",
    "    region_stats_552[j,:,4] = area\n",
    "    \n",
    "region_stats_673 = np.zeros((len(calims_673_tot),regions.shape[0],5))\n",
    "for j, path in zip(range(len(calims_673_tot)),calims_673_tot):\n",
    "    fname = os.path.join(path,'itk/region_stats.csv')\n",
    "    data = np.loadtxt(fname,delimiter=',',skiprows=1)\n",
    "    regions = data[:,0]\n",
    "    area = data[:,1]\n",
    "    chAF = data[:,2]\n",
    "    ch552 = data[:,5]\n",
    "    ch673 = data[:,8]\n",
    "    region_stats_673[j,:,0] = regions\n",
    "    region_stats_673[j,:,1] = chAF\n",
    "    region_stats_673[j,:,2] = ch552\n",
    "    region_stats_673[j,:,3] = ch673\n",
    "    region_stats_673[j,:,4] = area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allen-to-Experiment (Forward) Global Volume Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the growth/shrink factor for each experiment, as determined by the affine portion of the registration transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_552 = np.zeros((len(calims_552_tot),1))\n",
    "\n",
    "for j, path in zip(range(len(calims_552_tot)),calims_552_tot):\n",
    "    fname = os.path.join(path,'itk')\n",
    "    param_files = [os.path.join(fname,\"TransformParameters.{0}.txt\".format(i)) for i in range(4)]\n",
    "    parameter_object_init = itk.ParameterObject.New()\n",
    "    # print(temp)\n",
    "    parameter_object_init.ReadParameterFile(param_files)\n",
    "    transform_parameters = np.array(parameter_object_init.GetParameter(0,'TransformParameters'),dtype=float)\n",
    "    rotation = transform_parameters[:9].reshape(3, 3)\n",
    "    translation = transform_parameters[-3:][..., np.newaxis]\n",
    "    reg_affine: np.ndarray = np.append(rotation, translation, axis=1)\n",
    "    reg_affine = np.append(reg_affine, [[0,0,0,1]], axis=0)\n",
    "\n",
    "    vol_552[j] = np.linalg.det(rotation)\n",
    "    \n",
    "vol_673 = np.zeros((len(calims_673_tot),1))\n",
    "\n",
    "for j, path in zip(range(len(calims_673_tot)),calims_673_tot):\n",
    "    fname = os.path.join(path,'itk')\n",
    "    param_files = [os.path.join(fname,\"TransformParameters.{0}.txt\".format(i)) for i in range(4)]\n",
    "    parameter_object_init = itk.ParameterObject.New()\n",
    "    # print(temp)\n",
    "    parameter_object_init.ReadParameterFile(param_files)\n",
    "    transform_parameters = np.array(parameter_object_init.GetParameter(0,'TransformParameters'),dtype=float)\n",
    "    rotation = transform_parameters[:9].reshape(3, 3)\n",
    "    translation = transform_parameters[-3:][..., np.newaxis]\n",
    "    reg_affine: np.ndarray = np.append(rotation, translation, axis=1)\n",
    "    reg_affine = np.append(reg_affine, [[0,0,0,1]], axis=0)\n",
    "\n",
    "    vol_673[j] = np.linalg.det(rotation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_and_display_calibrations_cross_channel(stats_ch1,vols_ch1,im_paths_ch1,animals_ch1,\n",
    "                                                title=['Insert Channel 1 Name Here','Insert Channel 1 Name Here'],axes=None,same_ch_flag=True,\n",
    "                                                stats_ch2=None,vols_ch2=None,im_paths_ch2=None,animals_ch2=None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "\n",
    "        Function to calculate the covariance of regional intensity across brains.\n",
    "        This is calculated in three contexts:\n",
    "            Raw intensity\n",
    "            Global-volume-corrected intensity\n",
    "            Regional-volume-corrected intensity\n",
    "        The covraiance coefficients are plotted as heatmaps and returned as numpy arrays\n",
    "        \n",
    "        Variables:\n",
    "\n",
    "            stats_ch1 : numpy array\n",
    "                        Array containing the regional statistics as measured by aligning\n",
    "                        the experiment to the Allen atlas and computing the intensity and volume\n",
    "                        \n",
    "            vols_ch1 : numpy array\n",
    "                        Array containing the percent volume change for each experiment calculated\n",
    "                        from the affine matrix in the registration transformation\n",
    "            \n",
    "            im_paths_ch1 : string list\n",
    "                        File paths to the folder containing each experiment\n",
    "                            \n",
    "            animals_ch1 : string list\n",
    "                        Animal names for each experiment\n",
    "                        \n",
    "            title : string list\n",
    "                        Channel names for each channel under consieration\n",
    "                        \n",
    "            axes : pyplot figure\n",
    "                        Figure axes onto which the heatmaps will be plotted\n",
    "                        Can be omitted and a new figure will be created\n",
    "                        \n",
    "            same_ch_flag : bool\n",
    "                        Flag to indicate if the covariance corresponds to a cross- or auto-correlation\n",
    "                        If True, the inputs for \"ch1\" above will be copied to \"ch2\".\n",
    "                        If False, the user function expects similar inputs for \n",
    "                        stats_ch2, vols_ch2, im_paths_ch2, and animals_ch2 as described above for ch1.\n",
    "                        \n",
    "            stats_ch2, vols_ch2, im_paths_ch2, animals_ch2 : numpy array, numpy array, string list, string list\n",
    "                        These are equivalent to the inputs for ch2. Must be specified in the function call\n",
    "                        as variable name / value pairs: e.g. \n",
    "                        function(... , stats_ch2 = input_stats_array, ...)\n",
    "\n",
    "                \n",
    "    \"\"\"\n",
    "\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(nrows=1,ncols=3,figsize=(15,5))\n",
    "        \n",
    "    if same_ch_flag:\n",
    "        stats_ch2=np.copy(stats_ch1)\n",
    "        vols_ch2=np.copy(vols_ch1)\n",
    "        im_paths_ch2=im_paths_ch1.copy()\n",
    "        animals_ch2=animals_ch1.copy()\n",
    "        ch1_id = 2\n",
    "        ch2_id = 2\n",
    "    else:\n",
    "        ch1_id = 2\n",
    "        ch2_id = 3\n",
    "    \n",
    "    ### Raw Intensity\n",
    "    coeffs_raw = np.zeros((stats_ch1.shape[0],stats_ch2.shape[0]))\n",
    "    for j in range(stats_ch1.shape[0]):\n",
    "        X = stats_ch1[j,:,ch1_id].reshape(-1,1)\n",
    "        for jj in range(stats_ch2.shape[0]):\n",
    "            y = stats_ch2[jj,:,ch2_id].reshape(-1,1)\n",
    "            reg=LR().fit(X,y)\n",
    "            reg.score(X,y)\n",
    "            coeffs_raw[j,jj] = reg.coef_[0,0]\n",
    "    \n",
    "    ### Global Volume Correction\n",
    "    coeffs_global = np.zeros((stats_ch1.shape[0],stats_ch2.shape[0]))\n",
    "    for j in range(stats_ch1.shape[0]):\n",
    "        X = stats_ch1[j,:,ch1_id].reshape(-1,1)*vols_ch1[j]\n",
    "        for jj in range(stats_ch2.shape[0]):\n",
    "            y = stats_ch2[jj,:,ch2_id].reshape(-1,1)*vols_ch2[jj]\n",
    "            reg=LR().fit(X,y)\n",
    "            reg.score(X,y)\n",
    "            coeffs_global[j,jj] = reg.coef_[0,0]\n",
    "    \n",
    "    ### Regional Volume Correction\n",
    "    ### Ignore the deprecation warning... \n",
    "    \n",
    "    if same_ch_flag:\n",
    "        ch1_id = 1\n",
    "        ch2_id = 1\n",
    "    else:\n",
    "        ch1_id = 1\n",
    "        ch2_id = 2\n",
    "        \n",
    "    for_ints_ch1 = []\n",
    "    inv_ints_ch1 = []\n",
    "    ints_ch1 = []\n",
    "    ids_ch1 = []\n",
    "    for path in im_paths_ch1:\n",
    "        forward_regions = pd.read_csv(os.path.join(path,'itk','region_stats.csv'))\n",
    "        inverse_regions = pd.read_csv(os.path.join(path,'invert_test','region_stats.csv'))\n",
    "\n",
    "        intersect_ids = np.intersect1d(forward_regions.Region,inverse_regions.Region,return_indices=True)\n",
    "\n",
    "        forward_int = np.array(forward_regions.mean_ch1)\n",
    "        inverse_int = np.array(inverse_regions.intensity_mean)\n",
    "        \n",
    "        inv_int = np.zeros((len(inverse_int),3))\n",
    "        for jj in range(len(inverse_int)):\n",
    "            temp = inverse_int[jj]\n",
    "            n = 0\n",
    "            for nn, j in enumerate(temp.split()):\n",
    "                if j != r'[' and j != r']':\n",
    "                    temp1 = re.findall(r\"(?<!\\d|\\.)\\d+(?:\\.\\d+)?\",j)\n",
    "                    inv_int[jj,n] = np.squeeze(temp1[0]).astype(float)\n",
    "                    n += 1\n",
    "\n",
    "        X = np.array(forward_regions.N)[intersect_ids[1]]\n",
    "        Y = np.array(inverse_regions.N)[intersect_ids[2]]\n",
    "        for_ints_ch1.append(forward_int[intersect_ids[1]])\n",
    "        inv_ints_ch1.append(inv_int[intersect_ids[2],ch1_id])\n",
    "        ids_ch1.append(intersect_ids[0])\n",
    "\n",
    "        ints_ch1.append(np.array(inv_int[intersect_ids[2],ch1_id])*(Y/X))\n",
    "        \n",
    "    if same_ch_flag:\n",
    "        for_ints_ch2 = for_ints_ch1.copy()\n",
    "        inv_ints_ch2 = inv_ints_ch1.copy()\n",
    "        ints_ch2 = ints_ch1.copy()\n",
    "        ids_ch2 = ids_ch1.copy()\n",
    "    else:\n",
    "        for_ints_ch2 = []\n",
    "        inv_ints_ch2 = []\n",
    "        ints_ch2 = []\n",
    "        ids_ch2 = []\n",
    "        for path in im_paths_ch2:\n",
    "            forward_regions = pd.read_csv(os.path.join(path,'itk','region_stats.csv'))\n",
    "            inverse_regions = pd.read_csv(os.path.join(path,'invert_test','region_stats.csv'))\n",
    "\n",
    "            intersect_ids = np.intersect1d(forward_regions.Region,inverse_regions.Region,return_indices=True)\n",
    "\n",
    "            forward_int = np.array(forward_regions.mean_ch1)\n",
    "            inverse_int = np.array(inverse_regions.intensity_mean)\n",
    "            \n",
    "            inv_int = np.zeros((len(inverse_int),3))\n",
    "            for jj in range(len(inverse_int)):\n",
    "                temp = inverse_int[jj]\n",
    "                n = 0\n",
    "                for nn, j in enumerate(temp.split()):\n",
    "                    if j != r'[' and j != r']':\n",
    "                        temp1 = re.findall(r\"(?<!\\d|\\.)\\d+(?:\\.\\d+)?\",j)\n",
    "                        inv_int[jj,n] = np.squeeze(temp1[0]).astype(float)\n",
    "                        n += 1\n",
    "\n",
    "            X = np.array(forward_regions.N)[intersect_ids[1]]\n",
    "            Y = np.array(inverse_regions.N)[intersect_ids[2]]\n",
    "            for_ints_ch2.append(forward_int[intersect_ids[1]])\n",
    "            inv_ints_ch2.append(inv_int[intersect_ids[2],ch2_id])\n",
    "            ids_ch2.append(intersect_ids[0])\n",
    "\n",
    "            ints_ch2.append(np.array(inv_int[intersect_ids[2],ch2_id])*(Y/X))\n",
    "\n",
    "    coeffs_region = np.zeros((len(im_paths_ch1),len(im_paths_ch2)))\n",
    "    for j in range(len(im_paths_ch1)):\n",
    "        X = ints_ch1[j].flatten()[...,np.newaxis]\n",
    "        id_X = ids_ch1[j]\n",
    "        for jj in range(len(im_paths_ch2)):\n",
    "            y = ints_ch2[jj].flatten()[...,np.newaxis]\n",
    "            id_Y = ids_ch2[jj]\n",
    "            id_inter = np.intersect1d(id_X,id_Y,return_indices=True)\n",
    "            reg = LR().fit(X[id_inter[1]], y[id_inter[2]])\n",
    "            coeffs_region[j,jj] = reg.coef_[0]\n",
    "\n",
    "            \n",
    "    \n",
    "    ### Plot the covariances\n",
    "    axes[0].imshow(coeffs_raw,cmap=obj, vmin=np.amin([np.amin(coeffs_raw),np.amin(coeffs_global),np.amin(coeffs_region)]), \n",
    "                 vmax=np.amax([np.amax(coeffs_raw),np.amax(coeffs_global),np.amax(coeffs_region)]))\n",
    "    for j in range(coeffs_raw.shape[0]):\n",
    "        for jj in range(coeffs_raw.shape[0]):\n",
    "            axes[0].text(j-0.25, jj+0.1, f'{coeffs_raw[jj,j]:.3f}',c='k')\n",
    "    # plt.colorbar()\n",
    "    axes[0].set_title('Raw Intensity ')\n",
    "    axes[0].set_xticks(np.arange(stats_ch1.shape[0]),animals_ch1,rotation=45,ha='right');\n",
    "    axes[0].set_yticks(range(stats_ch2.shape[0]),animals_ch2,rotation=45);\n",
    "            \n",
    "    axes[1].imshow(coeffs_global,cmap=obj, vmin=np.amin([np.amin(coeffs_raw),np.amin(coeffs_global),np.amin(coeffs_region)]), \n",
    "                 vmax=np.amax([np.amax(coeffs_raw),np.amax(coeffs_global),np.amax(coeffs_region)]))\n",
    "    for j in range(coeffs_global.shape[0]):\n",
    "        for jj in range(coeffs_global.shape[0]):\n",
    "            axes[1].text(j-0.25, jj+0.1, f'{coeffs_global[jj,j]:.3f}',c='k')\n",
    "    # plt.colorbar()\n",
    "    # axes[1].set_title(title+' vs ' + title + ' Regression Coefficient')\n",
    "    axes[1].set_title('Global Volume Correction')\n",
    "    axes[1].set_xticks(np.arange(stats_ch1.shape[0]),animals_ch1,rotation=45,ha='right');\n",
    "    axes[1].set_yticks(range(stats_ch2.shape[0]),[],rotation=45);\n",
    "\n",
    "    pcm=axes[2].imshow(coeffs_region,cmap=obj, vmin=np.amin([np.amin(coeffs_raw),np.amin(coeffs_global),np.amin(coeffs_region)]), \n",
    "                 vmax=np.amax([np.amax(coeffs_raw),np.amax(coeffs_global),np.amax(coeffs_region)]))\n",
    "    fig.colorbar(pcm, ax=axes[:])\n",
    "\n",
    "    for i in range(coeffs_region.shape[0]):\n",
    "        for j in range(coeffs_region.shape[1]):\n",
    "            text = plt.text(j, i, f\"{coeffs_region[i, j]:.3f}\",\n",
    "                        ha='center', va='center', color='k')\n",
    "    axes[2].set_title('Regional Volume Correction')\n",
    "    axes[2].set_xticks(np.arange(stats_ch1.shape[0]),animals_ch1,rotation=45,ha='right');\n",
    "    axes[2].set_yticks(range(stats_ch2.shape[0]),[],rotation=45);\n",
    "    \n",
    "    fig.suptitle(title[0] + ' vs ' + title[1] + ' Covariance', fontsize=16)\n",
    "    \n",
    "    return np.vstack((coeffs_raw, coeffs_global, coeffs_region))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 552 vs 552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceoffs_552 = calc_and_display_calibrations_cross_channel(region_stats_552,vol_552,calims_552_tot,ANMS_552,title=['552','552'],same_ch_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 673 vs 673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceoffs_673 = calc_and_display_calibrations_cross_channel(region_stats_673,vol_673,calims_673_tot,ANMS_673,title=['673','673'],same_ch_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 552 vs 673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceoffs = calc_and_display_calibrations_cross_channel(region_stats_552,vol_552,calims_552_tot,ANMS_552,\n",
    "                                                     title=['552','673'],same_ch_flag=False,\n",
    "                                                     stats_ch2=region_stats_673,vols_ch2=vol_673,im_paths_ch2=calims_673_tot,animals_ch2=ANMS_673)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
